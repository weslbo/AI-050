{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use your own data with Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run library.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Azure OpenAI Service offers the capability to tailor OpenAI models (like GPT-3) to specific needs using two primary methodologies: Fine-tuning and using custom data for prompt engineering. Here’s a breakdown of the two concepts:\n",
       "\n",
       "### Fine-Tuning:\n",
       "\n",
       "**Fine-tuning** is a process where an existing pre-trained model is further trained on a specific dataset. This process adjusts the model’s weights based on the new data, allowing it to perform better on tasks related to the data provided.\n",
       "\n",
       "#### Key Aspects:\n",
       "1. **Specialization**: Fine-tuning allows the model to understand and generate responses that are highly specific to your domain, terminology, or style.\n",
       "2. **Data Requirements**: Requires a substantial and high-quality dataset relevant to the specific use-case for effective fine-tuning.\n",
       "3. **Time & Resources**: The process can be resource-intensive, requiring computational resources and time to adjust the model appropriately.\n",
       "4. **Performance**: Once fine-tuned, the model can perform very specialized tasks, offering significant improvements in areas like accuracy and relevance for the specified use-case.\n",
       "\n",
       "### Using Your Own Data:\n",
       "\n",
       "**Using your own data** in this context typically means customizing your prompts and responses to leverage the context provided by your data during runtime without altering the underlying model's weights.\n",
       "\n",
       "#### Key Aspects:\n",
       "1. **Prompt Engineering**: Crafting prompts that include relevant data points or context to guide the model’s responses effectively.\n",
       "2. **Flexibility**: This method doesn’t require changes to the model itself. Instead, it leverages the ability of the model to understand and generate language based on the inputs it receives.\n",
       "3. **Speed & Cost**: Less resource-intensive compared to fine-tuning. It can be done quickly and usually at lower costs since it doesn’t require re-training the model.\n",
       "4. **Real-Time Adaptability**: Can be particularly useful for applications where the data or context is dynamic and frequently changing.\n",
       "\n",
       "### Example Scenario:\n",
       "\n",
       "#### Fine-Tuning:\n",
       "An e-commerce company wants the model to generate very specific product descriptions that match their brand voice and include technical specifications with precision. They could fine-tune GPT-3 using a large dataset of their previous product descriptions.\n",
       "\n",
       "#### Using Your Own Data:\n",
       "For a customer service chatbot, the company could use prompt engineering. Each time a user asks a question, the system retrieves relevant snippets from a customer support database to include in the prompt. The pre-trained model then uses this context to generate an appropriate response.\n",
       "\n",
       "### Choosing Between Fine-Tuning and Using Your Own Data:\n",
       "\n",
       "1. **Use Fine-Tuning When:**\n",
       "   - You have a substantial and high-quality dataset.\n",
       "   - You need domain-specific language and responses.\n",
       "   - The use-case demands highly specialized or accurate outputs.\n",
       "\n",
       "2. **Use Your Own Data When:**\n",
       "   - You need a flexible, quick solution.\n",
       "   - The context or required information changes frequently.\n",
       "   - You have limited resources for training models.\n",
       "   - You need real-time adaptability and lower operational costs.\n",
       "\n",
       "By evaluating the specific needs and constraints of your application, you can determine the most effective approach to leverage the capabilities of Azure OpenAI models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%chat \n",
    "\n",
    "How can Azure OpenAI use your own data? Explain the concepts of Fine-tuning vs. using your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Below is a step-by-step guide on how to integrate Azure OpenAI with your data for more grounded and relevant responses:\n",
       "\n",
       "### Step 1: Set Up Your Data Source\n",
       "First, you need to identify and set up your data source. This could be databases, file storage systems, APIs, or any other data repositories where your information resides.\n",
       "\n",
       "1. **Choose the Data Source Type**: Determine whether you'll be using SQL databases, NoSQL databases, Azure Blob Storage, Azure Cosmos DB, or any other system compatible with Azure.\n",
       "2. **Connect Data to Azure**: Set up the necessary connections to allow Azure services to access your data. For Azure-native solutions like Azure SQL Database or Blob Storage, this might simply be a matter of configuring access within the Azure Portal.\n",
       "\n",
       "### Step 2: Load Data into Azure for OpenAI\n",
       "There are several methods to load your data into Azure:\n",
       "\n",
       "1. **Using Azure Data Factory**:  \n",
       "   - Create a new Data Pipeline.\n",
       "   - Select your data source as the input.\n",
       "   - Choose a suitable sink (e.g., Azure Blob Storage, Azure Data Lake Storage, etc.).\n",
       "   - Run the pipeline to transfer data.\n",
       "\n",
       "2. **Direct Upload via Azure Portal**: Use Azure Storage Explorer or the Azure Portal directly to upload your files if you're working with files.\n",
       "\n",
       "3. **API Integration**: \n",
       "   - If your data source is an API, ensure it’s accessible via a secure endpoint.\n",
       "   - Azure Logic Apps or Azure Functions can be used to connect to the API and pull data into Azure services.\n",
       "\n",
       "### Step 3: Configure the Studio or Your App to Connect to the Data Source\n",
       "To make Azure OpenAI use your data, you'll often use Azure Cognitive Search as an intermediary to index your data, which the model can then query.\n",
       "\n",
       "1. **Set Up Azure Cognitive Search**:  \n",
       "   - Create a search service in the Azure portal.\n",
       "   - Define your index based on the structure of your data.\n",
       "   - Feed your data into the Azure Cognitive Search index using data import tools.\n",
       "\n",
       "2. **Connect Azure OpenAI Service**:  \n",
       "   - Ensure that your Azure Cognitive Search service is properly configured and running.\n",
       "   - In your development environment (e.g., using Azure ML Studio, SDKs), connect to your Azure Cognitive Search service to fetch data when required.\n",
       "\n",
       "### Step 4: Use the Azure OpenAI Model with Your Data for Grounding\n",
       "\n",
       "1. **Invoke the OpenAI Model**:  \n",
       "   - When setting up calls to the Azure OpenAI model, route queries through Azure Cognitive Search.\n",
       "   - Adapt your application logic to handle pre-processing and post-processing of data.\n",
       "\n",
       "2. **Ground Responses with Indexed Data**:  \n",
       "   - Use the search index to find relevant data that can be fed as context to the OpenAI model.\n",
       "   - You can set up fine-tuning or prompt engineering mechanisms to ensure the relevant data improves the model's responses.\n",
       "\n",
       "### Example Workflow:\n",
       "```python\n",
       "import openai\n",
       "from azure.core.credentials import AzureKeyCredential\n",
       "from azure.search.documents import SearchClient\n",
       "\n",
       "# Setting up Azure Cognitive Search client\n",
       "search_client = SearchClient(endpoint=\"https://<your-search-service>.search.windows.net\",\n",
       "                             index_name=\"your-index-name\",\n",
       "                             credential=AzureKeyCredential(\"<your-api-key>\"))\n",
       "\n",
       "def search_and_generate_response(query):\n",
       "    # Step A: Perform a search query\n",
       "    search_results = search_client.search(query)\n",
       "  \n",
       "    # Extract relevant data from search results\n",
       "    context = \" \".join([result[\"content\"] for result in search_results])\n",
       "    \n",
       "    # Step B: Use OpenAI model to generate a response\n",
       "    response = openai.Completion.create(\n",
       "        engine=\"davinci-codex\",\n",
       "        prompt=f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
       "        max_tokens=150\n",
       "    )\n",
       "  \n",
       "    return response[\"choices\"][0][\"text\"]\n",
       "\n",
       "# Use the function to get a response\n",
       "query = \"How does Azure Cognitive Search integrate with OpenAI?\"\n",
       "response = search_and_generate_response(query)\n",
       "print(response)\n",
       "```\n",
       "\n",
       "By properly setting up Azure services and integrating them with OpenAI, you can ensure that the model's responses are enriched and grounded with your own data, leading to more accurate and contextually relevant outputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%chat\n",
    "\n",
    "Explain how Azure OpenAI can use your data by following these steps:\n",
    "\n",
    "1. Set up your data source\n",
    "2. Configure the studio or your app to connect to that data source\n",
    "3. Use the Azure OpenAI model, which now uses your data for grounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The correct answer is:\n",
       "\n",
       "- Use supported AI chat models that can reference specific sources of data\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "Azure OpenAI on your data allows developers to integrate powerful AI chat models provided by OpenAI with their own specific data sources. This enables the AI chat models to reference and make use of the specific data provided by the user, which can enhance the relevance and accuracy of the responses generated by the models. These capabilities are especially useful in scenarios where the AI needs to interact with domain-specific information or proprietary datasets.\n",
       "\n",
       "The other options, \"Create their own AI chat models\" and \"Access Azure OpenAI without an approved subscription,\" are incorrect because:\n",
       "\n",
       "1. **Create their own AI chat models**: While developers can fine-tune existing models using their data, Azure OpenAI service primarily focuses on providing access to pre-built models from OpenAI rather than facilitating the creation of entirely new models from scratch by users.\n",
       "   \n",
       "2. **Access Azure OpenAI without an approved subscription**: Azure OpenAI requires an approved subscription due to compliance, security, and usage considerations. Gaining access typically involves an approval process to ensure appropriate use and management of the powerful AI capabilities provided by OpenAI through Azure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%chat\n",
    "\n",
    "What does Azure OpenAI on your data enable developers to do?\n",
    "\n",
    "- Create their own AI chat models \n",
    "- Access Azure OpenAI without an approved subscription\n",
    "- Use supported AI chat models that can reference specific sources of data\n",
    "\n",
    "Choose the correct answer and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The correct answer is:\n",
       "\n",
       "- Using Azure AI Studio to create the search resource and index\n",
       "\n",
       "### Explanation:\n",
       "\n",
       "Azure AI Studio simplifies the process of integrating data sources with Azure OpenAI, offering a user-friendly interface where you can effortlessly create search resources and manage indexes. This ensures an optimized and streamlined workflow for adding data and allows you to leverage Azure's robust search and indexing capabilities effectively. Moreover, Azure AI Studio provides built-in tools and configurations tailored for a seamless experience, which might be more challenging and error-prone when manually connecting to files in a storage account without using Azure AI Studio.\n",
       "\n",
       "Using any data source option available can be too general and might not utilize the specific advantages offered by Azure AI Studio. Therefore, using Azure AI Studio is the recommended approach for its integrated features and ease of use."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%chat\n",
    "\n",
    "What is the recommended way to add data when using Azure OpenAI on your data?\n",
    "\n",
    "- Using any data source option available for Azure OpenAI on your data\n",
    "- Using Azure AI Studio to create the search resource and index\n",
    "- Connecting to files in a storage account without using Azure AI Studio\n",
    "\n",
    "Choose the correct answer and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The correct answer is:\n",
       "\n",
       "- Break down the task and use chain of thought prompting\n",
       "\n",
       "### Explanation:\n",
       "\n",
       "**Chain of Thought Prompting:**\n",
       "- **Why it's effective:** By breaking down complex tasks into smaller, manageable steps, you guide the model through the reasoning process. This helps the model better understand the problem at hand and generate more accurate and relevant responses.\n",
       "- **Example:** If you're solving a math problem, instead of asking the model to solve it directly, you could guide it step-by-step through each part of the problem. This mimics human thinking and leverages the model's pattern recognition abilities to achieve better results.\n",
       "\n",
       "### Why the other options are less preferable:\n",
       "\n",
       "1. **Include as much conversation history as possible in your call:**\n",
       "   - **Downside:** Including too much conversation history can lead to a bloated context window, potentially overwhelming the model and reducing its effectiveness. Azure OpenAI, like other instances of GPT-3/4, has a token limit, and exceeding that can result in truncated input, which might omit crucial information.\n",
       "  \n",
       "2. **Use a single long prompt to provide all necessary information:**\n",
       "   - **Downside:** A single long prompt can be too unwieldy and might include superfluous details that do not directly aid in resolving the task. It can also risk hitting token limits more quickly, constraining the model's ability to generate a complete and coherent response. It is better to provide information incrementally and relevantly to the task at hand.\n",
       "  \n",
       "### Summary\n",
       "By breaking down tasks and using chain of thought prompting, you align more closely with the model's strengths – logical progression and contextual understanding. The other options carry risks of overwhelming the model or exceeding operational token limits, which could hinder performance and accuracy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%chat\n",
    "\n",
    "What are some recommended prompt engineering techniques when using Azure OpenAI on your own data?\n",
    "\n",
    "- Break down the task and use chain of thought prompting\n",
    "- Include as much conversation history as possible in your call\n",
    "- Use a single long prompt to provide all necessary information\n",
    "\n",
    "Choose the correct answer and explain why"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
